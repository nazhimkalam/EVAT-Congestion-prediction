{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a472e11e",
   "metadata": {},
   "source": [
    "\n",
    "# Station Time‑Series → Preprocessed Dataset (Repro Notebook)\n",
    "\n",
    "This notebook reconstructs a clean, reproducible pipeline that **preprocesses** `station_timeseries.csv` into a model‑ready dataset.\n",
    "It also **audits** the uploaded `pre-processed-dataset.csv` to verify whether it could have been produced directly from the provided time‑series file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_PATH = Path(\"/mnt/data/station_timeseries.csv\")\n",
    "PROC_UPLOADED_PATH = Path(\"/mnt/data/pre-processed-dataset.csv\")  # Uploaded reference (session-level)\n",
    "OUT_PATH = Path(\"/mnt/data/pre-processed-timeseries-from-station.csv\")  # Our reproducible output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683501fa",
   "metadata": {},
   "source": [
    "## 1) Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw = pd.read_csv(RAW_PATH)\n",
    "raw_head = raw.head()\n",
    "raw_shape = raw.shape\n",
    "raw_dtypes = raw.dtypes\n",
    "\n",
    "print(\"RAW shape:\", raw_shape)\n",
    "print(\"RAW dtypes:\")\n",
    "print(raw_dtypes)\n",
    "raw_head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e9e6f",
   "metadata": {},
   "source": [
    "## 2) Audit the uploaded `pre-processed-dataset.csv` (schema check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637817ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uploaded = pd.read_csv(PROC_UPLOADED_PATH)\n",
    "up_head = uploaded.head()\n",
    "up_shape = uploaded.shape\n",
    "up_dtypes = uploaded.dtypes\n",
    "\n",
    "print(\"UPLOADED shape:\", up_shape)\n",
    "print(\"UPLOADED dtypes:\")\n",
    "print(up_dtypes.head(20))\n",
    "up_head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cb203",
   "metadata": {},
   "source": [
    "\n",
    "**Observation:** The uploaded file contains **session-level fields** like `sessionId`, `created`, `ended`, `kwhTotal`, etc. \n",
    "These fields **do not exist** in `station_timeseries.csv`, which has only `stationId`, `timestamp`, `arrivals`, `departures`, and `occupancy`.  \n",
    "Therefore, the uploaded `pre-processed-dataset.csv` could **not** have been produced solely from `station_timeseries.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424680b1",
   "metadata": {},
   "source": [
    "## 3) Clean & standardise `station_timeseries.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Parse schema ---\n",
    "expected_cols = ['stationId', 'timestamp', 'arrivals', 'departures', 'occupancy']\n",
    "missing_cols = [c for c in expected_cols if c not in raw.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing expected columns: {missing_cols}\")\n",
    "\n",
    "# --- Basic cleaning ---\n",
    "df = raw.copy()\n",
    "\n",
    "# Parse timestamp to datetime (UTC assumed; adjust if needed)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce', utc=True)\n",
    "\n",
    "# Drop rows with invalid timestamps or stationId\n",
    "df = df.dropna(subset=['timestamp', 'stationId'])\n",
    "\n",
    "# Ensure numeric types\n",
    "for col in ['arrivals', 'departures', 'occupancy']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Optional: remove obviously corrupt values (e.g., negative counts)\n",
    "for col in ['arrivals', 'departures', 'occupancy']:\n",
    "    df = df[df[col].ge(0) | df[col].isna()]\n",
    "\n",
    "# Fill remaining NaNs with sensible defaults (0 for counts)\n",
    "df[['arrivals','departures','occupancy']] = df[['arrivals','departures','occupancy']].fillna(0).astype(int)\n",
    "\n",
    "df = df.sort_values(['stationId','timestamp']).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54f6be",
   "metadata": {},
   "source": [
    "## 4) Feature engineering (time parts, lags, rolling stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfe = df.copy()\n",
    "\n",
    "# Time parts\n",
    "dfe['date'] = dfe['timestamp'].dt.date\n",
    "dfe['hour'] = dfe['timestamp'].dt.hour\n",
    "dfe['dow'] = dfe['timestamp'].dt.dayofweek  # 0=Mon\n",
    "dfe['is_weekend'] = (dfe['dow'] >= 5).astype(int)\n",
    "\n",
    "# Lags per station\n",
    "dfe['arrivals_lag1'] = dfe.groupby('stationId')['arrivals'].shift(1)\n",
    "dfe['departures_lag1'] = dfe.groupby('stationId')['departures'].shift(1)\n",
    "dfe['occupancy_lag1'] = dfe.groupby('stationId')['occupancy'].shift(1)\n",
    "\n",
    "# Rolling means (3-step window)\n",
    "dfe['arrivals_roll3'] = dfe.groupby('stationId')['arrivals'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
    "dfe['departures_roll3'] = dfe.groupby('stationId')['departures'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
    "dfe['occupancy_roll3'] = dfe.groupby('stationId')['occupancy'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
    "\n",
    "# Net flow proxy\n",
    "dfe['net_flow'] = dfe['arrivals'] - dfe['departures']\n",
    "\n",
    "# One-hot day of week (Mon..Sun) for many ML models\n",
    "dow_dummies = pd.get_dummies(dfe['dow'], prefix='dow', drop_first=False)\n",
    "dfe = pd.concat([dfe, dow_dummies], axis=1)\n",
    "\n",
    "dfe.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1873d63",
   "metadata": {},
   "source": [
    "## 5) Optional resampling (hourly) — align uneven timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09687ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If timestamps are not perfectly regular, you can resample per-station to hourly bins.\n",
    "# Comment this cell out if the data is already regular.\n",
    "def resample_station(g, rule='1H'):\n",
    "    g = g.set_index('timestamp').sort_index()\n",
    "    # Sum flows; take last known occupancy\n",
    "    agg = g.resample(rule).agg({\n",
    "        'arrivals': 'sum',\n",
    "        'departures': 'sum',\n",
    "        'occupancy': 'last',\n",
    "        'hour': 'first',\n",
    "        'dow': 'first',\n",
    "        'is_weekend': 'first',\n",
    "        'arrivals_lag1':'last',\n",
    "        'departures_lag1':'last',\n",
    "        'occupancy_lag1':'last',\n",
    "        'arrivals_roll3':'last',\n",
    "        'departures_roll3':'last',\n",
    "        'occupancy_roll3':'last',\n",
    "        'net_flow':'sum',\n",
    "        **{c:'first' for c in dfe.columns if c.startswith('dow_')}\n",
    "    })\n",
    "    agg['stationId'] = g['stationId'].iloc[0]\n",
    "    return agg.reset_index()\n",
    "\n",
    "dfe_res = dfe.groupby('stationId', group_keys=False).apply(resample_station, rule='1H')\n",
    "\n",
    "# Recompute time parts after resample\n",
    "dfe_res['date'] = dfe_res['timestamp'].dt.date\n",
    "dfe_res['hour'] = dfe_res['timestamp'].dt.hour\n",
    "dfe_res['dow'] = dfe_res['timestamp'].dt.dayofweek\n",
    "dfe_res['is_weekend'] = (dfe_res['dow'] >= 5).astype(int)\n",
    "\n",
    "dfe_out = dfe_res.sort_values(['stationId','timestamp']).reset_index(drop=True)\n",
    "dfe_out.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c593267",
   "metadata": {},
   "source": [
    "## 6) Save preprocessed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select tidy column order\n",
    "time_cols = ['timestamp','date','hour','dow','is_weekend']\n",
    "base_cols = ['stationId','arrivals','departures','occupancy','net_flow']\n",
    "lag_cols = ['arrivals_lag1','departures_lag1','occupancy_lag1']\n",
    "roll_cols = ['arrivals_roll3','departures_roll3','occupancy_roll3']\n",
    "dow_cols = [c for c in dfe_out.columns if c.startswith('dow_')]\n",
    "\n",
    "cols = ['stationId'] + time_cols + base_cols + lag_cols + roll_cols + dow_cols\n",
    "cols = [c for c in cols if c in dfe_out.columns]  # safety\n",
    "\n",
    "final = dfe_out.loc[:, cols].copy()\n",
    "\n",
    "final.to_csv(OUT_PATH, index=False)\n",
    "OUT_PATH, final.shape, final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aec0d5",
   "metadata": {},
   "source": [
    "## 7) (Optional) Compare with the uploaded file to confirm mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uploaded_cols = set(pd.read_csv(PROC_UPLOADED_PATH, nrows=0).columns)\n",
    "ours_cols = set(final.columns)\n",
    "\n",
    "only_in_uploaded = sorted(list(uploaded_cols - ours_cols))[:30]\n",
    "only_in_ours = sorted(list(ours_cols - uploaded_cols))[:30]\n",
    "\n",
    "print(\"Columns only in uploaded (first 30):\", only_in_uploaded)\n",
    "print(\"Columns only in ours (first 30):\", only_in_ours)\n",
    "print(\"\\nConclusion: Different schemas — uploaded file is session-level; ours is time-series‑level.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
